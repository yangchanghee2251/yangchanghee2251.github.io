---
layout: single
title: "T2M-GPT 정리"
author_profile: true
categories:
  - human
toc_label: "목차"
toc: true
toc_sticky: true
sidebar:
    nav: "sidebar-category"
---


## T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations

CVPR2023에 Accept된 text to motion으로 video motion 결과를 얻음

## Abstact
텍스트에서 human motion을 생성하기 위해 Vector Quantised Variational AutoEncoder (VQ-VAE)를 만들고 Generative Pretrained Transforer(GPT)를 사용함. Motion VQ-VA는 Human motion data를 이산적인 코드 북으로 변환 다양한 텍스트 설명에 대해 고품질의 3D Human motion을 생성할 수 잇음을 보여줌.

## Intro
기존의 메소드는 3-stage 뿐 아니라 다양한 motion에서 실패한다고 언급한다.  
Stage 1에서는 stadard 1D convolutional network를 사용하는데 motion sequences를 discrete code에 mapping하고 Stage 2는 standard GPT모델을 사용하는데 text를 embedding한다.

## Method

### Motion VQ-VAE
VQ-VAE는 `Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017`에서 제안된 방법이고 discrete representations for generative models이다. motion seqeunce를 K code가 포함되며 훈련 가능하도록 만들었다.

<img src="../post_images/motionvq-vae.png" width="50%"  title="table1" alt=""/>  
이 논문에서 motion-VQ-VAE를 사용한 이유는 위에서 언급한 논문을 확인하면 알 수 있는데 기존의 VAE의 경우 posterior collapse라는 문제가 존재하는데 이는 인코더의 출력이 디코더에 의해서 무시되버리는 경우에 발생하고 VAE에서 흔히 발생하는 문제라고 한다. 또한 KL-divergence항이 loss 함수에 추가되는데 이는 사전 분포와 일치하도록 만들어서 다양성이 부족해지기 때문에 이산적으로 만들어서 훈련하도록 만드는 법을 제안했다. 이를 text2motion에 가져왔다고 생각하면 될 것 같다. 이렇게 latent space와 가장 일치하는 z를 찾아서 매핑하는 거라고 생각하면 된다.  

Quantization strategy Expoenetial moving average(EMA)를 사용해 smooth하게 만들어 줫음  

Architecture 간단히 Conv1D를 사용한 듯 하다.

### T2M-GPT

<img src="../post_images/t2m-gpt.png" width="50%"  title="table1" alt=""/>  
T2M-GPT의 경우 text to motion을 만드는 model로 위 그림과 같다. Text가 들어가면 CLIP을 통해 index를 뽑는데 이걸로 input이 들어간다. index의 경우 code book의 latent space인듯 하다. 이 모델의 장점은 또한 End를 넣어줘 sequence를 모델 스스로 만들도록 한다는 장점이 있다.

## Code
[여기](https://github.com/Mael-zys/T2M-GPT)에 있다. 다음 포스팅은 Code를 돌리는 법, 해석등이 될 것 같다.